# ðŸ”’ PromptGuard â€” Jailbreak Protection

While LlamaGuard focuses on unsafe **topics**, **PromptGuard** is designed to prevent **jailbreak attempts** â€” clever ways users might try to trick or bypass the modelâ€™s safety constraints.

For example:

* Trying to **reword** restricted questions.
* Using **indirect prompts** or fictional framing to elicit harmful content.
* Embedding unsafe requests inside benign-looking text.

PromptGuard is your **campus security system** for LLM behavior â€” making sure students canâ€™t find a backdoor to get the model to do things it shouldnâ€™t.