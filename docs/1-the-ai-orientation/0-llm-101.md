# 🧠 Deep Dive: How LLMs Generate Text

In Canopy AI, your assistant appears to think quickly and respond naturally. But what’s actually happening behind the scenes? This chapter explores the core mechanics that power LLMs during **inference** — the process of turning prompts into meaningful, human-like output.

We'll focus on the concepts that matter most for real-world deployment in educational settings.

---

## 🧱 What is a Token?

Tokens are the **smallest units of text** an LLM processes. A token might be a word, a piece of a word, or even punctuation.

Examples:
- `"The"` → 1 token
- `"unbelievable"` → 3 tokens (`"un"`, `"believ"`, `"able"`)

LLMs don’t read full sentences—they process token sequences. The total number of tokens affects:
- Memory usage
- Inference speed
- Output length

⚠️ **Tip**: More tokens = slower, more expensive inference.

<iframe
	src="https://agents-course-the-tokenizer-playground.static.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

## 🔮 Are LLMs Fixed or Do They Change Over Time?

LLMs are **frozen once trained**—they do **not learn** or update on the fly. Each time you send a prompt, they respond based on **pretrained knowledge** and context in the prompt.

However, outputs may differ due to:
- **Random sampling strategies**
- **Changes in prompts**
- **Different system instructions**

You can’t “teach” an LLM new facts mid-conversation unless it’s part of the prompt or a fine-tuned model.

---

> TODO: Instruct to the users to go to the CanopyUI and compare results with another colleague.

## 🔄 Next-Token Prediction: How LLMs Work

LLMs are **next-token machines**. At their core, they do one thing:  
👉 Predict the most likely next token based on everything they’ve seen so far.

For example:
> Input: "Photosynthesis is the process by which plants"  
> Prediction: `" convert sunlight into energy"`

This generation happens one token at a time, using **probabilities** and **context** to decide what comes next.

<h3>📝 Quiz: What does an LLM do during inference?</h3>
<ul>
  <li><button onclick="alert('✅ Correct! LLMs predict the next token based on prior context.')">Predict the most likely next token based on previous ones</button></li>
  <li><button onclick="alert('❌ Not quite. That’s text classification.')">Classify the topic of a sentence</button></li>
  <li><button onclick="alert('❌ Nope. LLMs don’t query databases unless you integrate retrieval.')">Retrieve facts from a database</button></li>
</ul>

## 👀 What is Attention?

**Attention** helps the model focus on the most relevant tokens in the input when generating output.

In the sentence:
> "When the student finished the exam, they felt relieved."

To predict "they", the model uses attention to relate it back to "the student".

Attention is why LLMs feel smart — it allows them to **track meaning and reference across long inputs**.

## 🧠 What is Context Length / Context Window?

The **context window** is how many tokens the model can “remember” at once.

Typical ranges:
- Small models: 2K–4K tokens
- Modern models: 8K–128K+ tokens
- Cutting-edge models: up to 1 million tokens (e.g., Qwen2.5-1M)

More context = better understanding of long documents or prior messages.  
But it comes at a cost:
- Slower performance
- More VRAM usage
- Higher latency

## ⚡ KV Cache: Making Inference Faster

As models generate tokens, they keep track of past computations using a **KV (Key-Value) Cache**.

Instead of recomputing attention for every previous token at each step, the model stores the intermediate results (keys and values) from earlier layers and reuses them as it continues generating.

Benefits:
- Avoids repeating expensive calculations
- Greatly improves decode speed
- Reduces latency for long responses
- Enables responsive UIs like Canopy AI’s streaming assistant

KV caching is why you see real-time streaming after the first token is generated—it drastically reduces the work needed per new token, especially for longer conversations or document processing.

## 💡 Prompting: How to Guide the Model

Prompting is how we shape model behavior. There are two key parts:
- **System Prompt**: Defines the assistant’s role (e.g., “You are a helpful tutor…”)
- **User Prompt**: The actual input or question

Small changes in wording can **dramatically** change the output. That’s why prompt engineering is crucial in education — it determines how clearly, accurately, and appropriately the model responds to students and instructors.

📚 We’ll dive much deeper into **prompt engineering strategies**, including real examples and hands-on practice, in the next chapters.

## 🚨 Hallucinations: When Models Make Stuff Up

LLMs sometimes **hallucinate** — confidently generate text that’s incorrect or fictional.

Why it happens:
- They optimize for coherence, not factual accuracy
- They don't “know” facts—they predict likely token sequences

Mitigation tips:
- Include accurate facts in the prompt
- Use guardrails (see below)
- Add retrieval or validation layers

## 🛡️ Guardrails: Controlling What Models Say

To keep your assistant safe and on-task, you can apply **guardrails**, such as:
- Prompt templates with strict instructions
- Output filters (block offensive or harmful content)
- External validation (e.g., fact-checking or classifiers)

For Canopy AI, these guardrails are essential to ensure alignment with educational standards.

## 📏 Key Inference Metrics

Understanding how your model performs helps you scale and troubleshoot.

| Metric                  | Meaning                                                      |
|-------------------------|--------------------------------------------------------------|
| **TTFT**                | Time to First Token – how fast the model starts responding   |
| **TPOT**                | Time Per Output Token – how fast each new token is generated |
| **Throughput**          | Number of parallel requests handled                          |
| **VRAM Usage**          | GPU memory required (↑ model size or context = ↑ memory)     |

These metrics help you balance latency vs. cost in OpenShift AI deployments.

## 📦 Model Sizes and GPU Needs

Model size matters—for performance *and* capability.

| Model Size     | Parameters | GPU Requirement            | Notes                            |
|----------------|------------|-----------------------------|----------------------------------|
| **<3B**         | Small      | 8–12GB VRAM (1 GPU)         | Lightweight and fast             |
| **7B–13B**      | Medium     | ≥24GB VRAM or quantization  | Balanced power vs. cost          |
| **>30B**        | Large      | Multi-GPU or high-end cards | Slower, but more context-aware   |

🧠 Larger models may be smarter, but smaller ones are often faster and easier to deploy.

## ✅ Summary

| Concept                | Key Idea                                                            |
|------------------------|---------------------------------------------------------------------|
| **Token**              | The basic unit of LLM input/output                                  |
| **Next-token machine** | LLMs predict one token at a time                                    |
| **Attention**          | Helps models focus on relevant words                                |
| **Context length**     | How much the model can "remember"                                   |
| **KV Cache**           | Speeds up generation by caching internal state                      |
| **Prompting**          | Guides model behavior through smart input design                    |
| **Hallucination**      | LLMs can generate plausible but wrong info                          |
| **Guardrails**         | Techniques to constrain model behavior and output                   |
| **TTFT & TPOT**        | Speed metrics for user experience                                   |
| **VRAM & Throughput**  | Resource and scalability metrics                                    |
| **Model size & GPU**   | Match model size to hardware capability and use case                |
