# 🧠 Deep Dive: How LLMs Generate Text

[🔝 Back to Overview](#contents)

In Canopy AI, your assistant appears to think quickly and respond naturally. But what's actually happening behind the scenes? This chapter explores the core mechanics that power LLMs during **inference** — the process of turning prompts into meaningful, human-like output.

We'll focus on the concepts that matter most for real-world deployment in educational settings.

## 📚 Contents

- [🧠 Deep Dive: How LLMs Generate Text](#-deep-dive-how-llms-generate-text)
  - [📚 Contents](#-contents)
  - [🧱 What is a Token?](#-what-is-a-token)
  - [🔮 Are LLMs Fixed or Do They Change Over Time?](#-are-llms-fixed-or-do-they-change-over-time)
  - [🔍 Hands-on Exercises](#-hands-on-exercises)
  - [🔄 Next-Token Prediction: How LLMs Work](#-next-token-prediction-how-llms-work)
  - [👀 What is Attention?](#-what-is-attention)
  - [🧠 What is Context Length / Context Window?](#-what-is-context-length--context-window)
  - [⚡ KV Cache: Making Inference Faster](#-kv-cache-making-inference-faster)
  - [💭 Prompting: How to Guide the Model](#-prompting-how-to-guide-the-model)
  - [🚨 Hallucinations: When Models Make Stuff Up](#-hallucinations-when-models-make-stuff-up)
  - [🛡️ Guardrails: Controlling What Models Say](#️-guardrails-controlling-what-models-say)
  - [📊 Key Inference Metrics](#-key-inference-metrics)
  - [📦 Model Sizes and GPU Needs](#-model-sizes-and-gpu-needs)
  - [✅ Summary](#-summary)
  - [📚 References](#-references)

---

## 🧱 What is a Token?

Tokens are the **smallest units of text** an LLM processes. A token might be a word, a piece of a word, or even punctuation.

Examples:
- `"The"` → 1 token
- `"unbelievable"` → 3 tokens (`"un"`, `"believ"`, `"able"`)

LLMs don't read full sentences—they process token sequences. The total number of tokens affects:
- Memory usage
- Inference speed
- Output length

⚠️ **Tip**: More tokens = slower, more expensive inference.

<iframe
	src="https://agents-course-the-tokenizer-playground.static.hf.space"
	frameborder="0"
	width="500"
	height="750"
	style="border: 1px solid #ccc; border-radius: 8px;"
	loading="lazy">
></iframe>

*The App is from [HuggingFace Learning Course](https://agents-course-the-tokenizer-playground.static.hf.space)*

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">🔤 Quiz: How do tokens impact LLM performance?</h3>

<style>
.quiz-container-tokens { position: relative; }
.quiz-option-tokens {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-tokens:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-tokens { display: none; }
.quiz-radio-tokens:checked + .quiz-option-tokens { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-tokens[value="wrong"]:checked + .quiz-option-tokens { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-tokens {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#tokens-correct:checked ~ .feedback-tokens.success { opacity: 1; }
#tokens-wrong1:checked ~ .feedback-tokens.error, #tokens-wrong2:checked ~ .feedback-tokens.error { opacity: 1; }
.feedback-tokens.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-tokens.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-tokens">

  <input type="radio" name="quiz-tokens" id="tokens-wrong1" class="quiz-radio-tokens" value="wrong">
  <label for="tokens-wrong1" class="quiz-option-tokens">🚀 More tokens always improve output quality</label>

  <input type="radio" name="quiz-tokens" id="tokens-correct" class="quiz-radio-tokens" value="correct">
  <label for="tokens-correct" class="quiz-option-tokens" data-correct="true">⚡ More tokens increase memory usage and slow down inference</label>
  
  <input type="radio" name="quiz-tokens" id="tokens-wrong2" class="quiz-radio-tokens" value="wrong">
  <label for="tokens-wrong2" class="quiz-option-tokens">🔢 Token count doesn't affect processing speed</label>

  <div class="feedback-tokens success">✅ <strong>Perfect!</strong> You understand that tokens directly impact performance and costs.</div>
  <div class="feedback-tokens error">❌ <strong>Not quite!</strong> Remember: more tokens = more memory + slower processing.</div>
</div>

</div>

[⬅️ Previous: Contents](#contents) | [➡️ Next: Are LLMs Fixed or Change Over Time?](#-are-llms-fixed-or-do-they-change-over-time)

---

## 🔮 Are LLMs Fixed or Do They Change Over Time?

[🔝 Back to Contents](#contents)

LLMs are **frozen once trained**—they do **not learn** or update on the fly. Each time you send a prompt, they respond based on **pretrained knowledge** and context in the prompt.

However, outputs may differ due to:
- **Random sampling strategies**
- **Changes in prompts**
- **Different system instructions**

You can't "teach" an LLM new facts mid-conversation unless it's part of the prompt or a fine-tuned model.

So let's do a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">🧠 Quiz: Do LLMs learn new information during conversations?</h3>

<style>
.quiz-container-learning { position: relative; }
.quiz-option-learning {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-learning:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-learning { display: none; }
.quiz-radio-learning:checked + .quiz-option-learning { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-learning[value="wrong"]:checked + .quiz-option-learning { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-learning {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#learning-correct:checked ~ .feedback-learning.success { opacity: 1; }
#learning-wrong1:checked ~ .feedback-learning.error, #learning-wrong2:checked ~ .feedback-learning.error { opacity: 1; }
.feedback-learning.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-learning.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-learning">

  <input type="radio" name="quiz-learning" id="learning-wrong1" class="quiz-radio-learning" value="wrong">
  <label for="learning-wrong1" class="quiz-option-learning">📚 Yes, they continuously update their knowledge from each conversation</label>

  <input type="radio" name="quiz-learning" id="learning-wrong2" class="quiz-radio-learning" value="wrong">
  <label for="learning-wrong2" class="quiz-option-learning">🔄 They learn gradually but only remember within the same session</label>
  
  <input type="radio" name="quiz-learning" id="learning-correct" class="quiz-radio-learning" value="correct">
  <label for="learning-correct" class="quiz-option-learning" data-correct="true">🔒 No, they are frozen after training and don't learn new information</label>

  <div class="feedback-learning success">✅ <strong>Exactly right!</strong> LLMs are fixed after training and only work with their pretrained knowledge plus current context.</div>
  <div class="feedback-learning error">❌ <strong>Think again!</strong> LLMs don't update or learn - they're frozen after training.</div>
</div>

</div>

---

[🔝 Back to Contents](#contents)

## 🔍 Hands-on Exercises

> **Exercise 1**: Memory Test
1. In Gradio, send a message asking "What did you learn today?"
2. Send a new message asking "What did I say in the last message?"
3. Compare this with how Canopy UI handles conversation memory

> **Exercise 2**: Consistency Check
1. Ask the model a specific question
2. Note the response
3. Ask the same question again
4. Compare the responses to understand how consistency works in LLMs

[🔝 Back to Contents](#contents)

[⬅️ Previous: Are LLMs Fixed?](#-are-llms-fixed-or-do-they-change-over-time) | [➡️ Next: Next-Token Prediction](#-next-token-prediction-how-llms-work)

---

## 🔄 Next-Token Prediction: How LLMs Work

[🔝 Back to Contents](#contents)

LLMs are **next-token machines**. At their core, they do one thing:  
👉 Predict the most likely next token based on everything they’ve seen so far.

For example:
> Input: "Photosynthesis is the process by which plants"  
> Prediction: `" convert sunlight into energy"`

This generation happens one token at a time, using **probabilities** and **context** to decide what comes next.

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">📝 Quiz: What does an LLM do during inference?</h3>

<style>
.quiz-container { position: relative; }
.quiz-option {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio { display: none; }
.quiz-radio:checked + .quiz-option { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio[value="wrong"]:checked + .quiz-option { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#correct:checked ~ .feedback.success { opacity: 1; }
#wrong1:checked ~ .feedback.error, #wrong2:checked ~ .feedback.error { opacity: 1; }
.feedback.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container">

  <input type="radio" name="quiz" id="wrong2" class="quiz-radio" value="wrong">
  <label for="wrong2" class="quiz-option">🗄️ Retrieve facts from a database</label>

  <input type="radio" name="quiz" id="correct" class="quiz-radio" value="correct">
  <label for="correct" class="quiz-option" data-correct="true">🎯 Predict the most likely next token based on previous ones</label>
  
  <input type="radio" name="quiz" id="wrong1" class="quiz-radio" value="wrong">
  <label for="wrong1" class="quiz-option">📊 Classify the topic of a sentence</label>

  <div class="feedback success">✅ <strong>Excellent!</strong> You understand how LLMs work during inference.</div>
  <div class="feedback error">❌ <strong>Try again!</strong> Think about what LLMs fundamentally do during text generation.</div>
</div>

</div>

[⬅️ Previous: Next-Token Prediction](#-next-token-prediction-how-llms-work) | [➡️ Next: What is Attention?](#-what-is-attention)

---

## 👀 What is Attention?

[🔝 Back to Contents](#contents)

**Attention** helps the model focus on the most relevant tokens in the input when generating output.

In the sentence:
> "When the student finished the exam, they felt relieved."

To predict "they", the model uses attention to relate it back to "the student".

Attention is why LLMs feel smart — it allows them to **track meaning and reference across long inputs**.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Visual Gif of Attention" width="60%">

*The video is from [HuggingFace Learning Course](https://huggingface.co/learn/llm-course/en/chapter1/8?fw=pt#the-role-of-attention)*


<div class="iframe-scroll-container" style="width: 100%; overflow-x: auto; margin: 20px 0;">
  <iframe
    src="https://yatheshr-bert-attention-visualizer.hf.space"
    width="100%"
    height="600px"
    frameborder="0"
    style="border: 1px solid #ddd; border-radius: 8px; min-width: 800px;"
    loading="lazy"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    sandbox="allow-scripts allow-same-origin allow-forms allow-popups allow-presentation">
  </iframe>
</div>

<p><em>Interactive attention visualization tool from <a href="https://huggingface.co/spaces/Yatheshr/bert_attention_visualizer" target="_blank">HuggingFace Spaces</a>. This tool helps visualize how BERT models use attention mechanisms to focus on different parts of the input text.</em></p>

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">👀 Quiz: What does attention help LLMs do?</h3>

<style>
.quiz-container-attention { position: relative; }
.quiz-option-attention {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-attention:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-attention { display: none; }
.quiz-radio-attention:checked + .quiz-option-attention { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-attention[value="wrong"]:checked + .quiz-option-attention { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-attention {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#attention-correct:checked ~ .feedback-attention.success { opacity: 1; }
#attention-wrong1:checked ~ .feedback-attention.error, #attention-wrong2:checked ~ .feedback-attention.error { opacity: 1; }
.feedback-attention.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-attention.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-attention">

  <input type="radio" name="quiz-attention" id="attention-wrong1" class="quiz-radio-attention" value="wrong">
  <label for="attention-wrong1" class="quiz-option-attention">🎯 Focus only on the most recent tokens in the sequence</label>

  <input type="radio" name="quiz-attention" id="attention-correct" class="quiz-radio-attention" value="correct">
  <label for="attention-correct" class="quiz-option-attention" data-correct="true">🔗 Focus on relevant tokens throughout the input to understand relationships</label>
  
  <input type="radio" name="quiz-attention" id="attention-wrong2" class="quiz-radio-attention" value="wrong">
  <label for="attention-wrong2" class="quiz-option-attention">📊 Calculate mathematical operations between tokens</label>

  <div class="feedback-attention success">✅ <strong>Great job!</strong> Attention helps models understand which parts of the input are most relevant for generating each output token.</div>
  <div class="feedback-attention error">❌ <strong>Try again!</strong> Think about how attention helps models understand relationships across the entire input.</div>
</div>

</div>


[⬅️ Previous: What is Attention?](#-what-is-attention) | [➡️ Next: What is Context Length?](#-what-is-context-length--context-window)

---

## 🧠 What is Context Length / Context Window?

[🔝 Back to Contents](#contents)

The **context window** is how many tokens the model can "remember" at once.

Typical ranges:
- Small models: 2K–4K tokens
- Modern models: 8K–128K+ tokens
- Cutting-edge models: up to 1 million tokens (e.g., Qwen2.5-1M)

More context = better understanding of long documents or prior messages.
But it comes at a cost:
- Slower performance
- More VRAM usage
- Higher latency

> TODO: Gradio App with a very short context window.

> TODO: Add a challenge to summarize some content that hits the context window max capacity

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">🧠 Quiz: What happens with larger context windows?</h3>

<style>
.quiz-container-context { position: relative; }
.quiz-option-context {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-context:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-context { display: none; }
.quiz-radio-context:checked + .quiz-option-context { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-context[value="wrong"]:checked + .quiz-option-context { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-context {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#context-correct:checked ~ .feedback-context.success { opacity: 1; }
#context-wrong1:checked ~ .feedback-context.error, #context-wrong2:checked ~ .feedback-context.error { opacity: 1; }
.feedback-context.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-context.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-context">

  <input type="radio" name="quiz-context" id="context-wrong1" class="quiz-radio-context" value="wrong">
  <label for="context-wrong1" class="quiz-option-context">🚀 Models run faster and use less memory</label>

  <input type="radio" name="quiz-context" id="context-wrong2" class="quiz-radio-context" value="wrong">
  <label for="context-wrong2" class="quiz-option-context">📈 Only output quality improves with no downsides</label>
  
  <input type="radio" name="quiz-context" id="context-correct" class="quiz-radio-context" value="correct">
  <label for="context-correct" class="quiz-option-context" data-correct="true">⚡ Better understanding but slower performance and higher VRAM usage</label>

  <div class="feedback-context success">✅ <strong>Spot on!</strong> Larger context windows provide better understanding but come with performance and resource trade-offs.</div>
  <div class="feedback-context error">❌ <strong>Not quite!</strong> Remember: larger context = better understanding but more resources needed.</div>
</div>

</div>

[⬅️ Previous: What is Context Length?](#-what-is-context-length--context-window) | [➡️ Next: KV Cache](#-kv-cache-making-inference-faster)

---

## ⚡ KV Cache: Making Inference Faster

[🔝 Back to Contents](#contents)

As models generate tokens, they keep track of past computations using a **KV (Key-Value) Cache**.

Instead of recomputing attention for every previous token at each step, the model stores the intermediate results (keys and values) from earlier layers and reuses them as it continues generating.

![KV Cache Autoregression Diagram - Shows how key-value caching optimizes token generation by storing and reusing previous computations](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/autoregression.png "KV Cache optimization during autoregressive text generation")

*Figure: KV Cache optimization during autoregressive text generation. Source: [HuggingFace Documentation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/kv-cache/autoregression.png)*

Benefits:
- Avoids repeating expensive calculations
- Greatly improves decode speed
- Reduces latency for long responses
- Enables responsive UIs like Canopy AI's streaming assistant

KV caching is why you see real-time streaming after the first token is generated—it drastically reduces the work needed per new token, especially for longer conversations or document processing.

So let's do a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">⚡ Quiz: What is the main benefit of KV Cache?</h3>

<style>
.quiz-container-kvcache { position: relative; }
.quiz-option-kvcache {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-kvcache:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-kvcache { display: none; }
.quiz-radio-kvcache:checked + .quiz-option-kvcache { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-kvcache[value="wrong"]:checked + .quiz-option-kvcache { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-kvcache {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#kvcache-correct:checked ~ .feedback-kvcache.success { opacity: 1; }
#kvcache-wrong1:checked ~ .feedback-kvcache.error, #kvcache-wrong2:checked ~ .feedback-kvcache.error { opacity: 1; }
.feedback-kvcache.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-kvcache.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-kvcache">

  <input type="radio" name="quiz-kvcache" id="kvcache-wrong1" class="quiz-radio-kvcache" value="wrong">
  <label for="kvcache-wrong1" class="quiz-option-kvcache">📊 It improves the quality of generated text</label>

  <input type="radio" name="quiz-kvcache" id="kvcache-wrong2" class="quiz-radio-kvcache" value="wrong">
  <label for="kvcache-wrong2" class="quiz-option-kvcache">🧠 It increases the model's context window capacity</label>
  
  <input type="radio" name="quiz-kvcache" id="kvcache-correct" class="quiz-radio-kvcache" value="correct">
  <label for="kvcache-correct" class="quiz-option-kvcache" data-correct="true">🚀 It avoids recomputing attention for previous tokens, speeding up generation</label>

  <div class="feedback-kvcache success">✅ <strong>Perfect!</strong> KV Cache stores previous computations to avoid redundant calculations during token generation.</div>
  <div class="feedback-kvcache error">❌ <strong>Think again!</strong> KV Cache is about performance optimization, not content quality or capacity.</div>
</div>

</div>

[⬅️ Previous: KV Cache](#-kv-cache-making-inference-faster) | [➡️ Next: Prompting](#-prompting-how-to-guide-the-model)

---

## 💭 Prompting: How to Guide the Model

[🔝 Back to Contents](#contents)

[🔝 Back to Contents](#contents)

Prompting is how we shape model behavior. There are two key parts:
- **System Prompt**: Defines the assistant’s role (e.g., “You are a helpful tutor…”)
- **User Prompt**: The actual input or question

Small changes in wording can **dramatically** change the output. That’s why prompt engineering is crucial in education — it determines how clearly, accurately, and appropriately the model responds to students and instructors.

📚 We’ll dive much deeper into **prompt engineering strategies**, including real examples and hands-on practice, in the next chapters.
<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">📝 Quiz: What's the difference between system and user prompts?</h3>

<style>
.quiz-container-prompting { position: relative; }
.quiz-option-prompting {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-prompting:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-prompting { display: none; }
.quiz-radio-prompting:checked + .quiz-option-prompting { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-prompting[value="wrong"]:checked + .quiz-option-prompting { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-prompting {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#prompting-correct:checked ~ .feedback-prompting.success { opacity: 1; }
#prompting-wrong1:checked ~ .feedback-prompting.error, #prompting-wrong2:checked ~ .feedback-prompting.error { opacity: 1; }
.feedback-prompting.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-prompting.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-prompting">

  <input type="radio" name="quiz-prompting" id="prompting-wrong1" class="quiz-radio-prompting" value="wrong">
  <label for="prompting-wrong1" class="quiz-option-prompting">🔄 System prompts are user questions and user prompts define the assistant's role</label>

  <input type="radio" name="quiz-prompting" id="prompting-correct" class="quiz-radio-prompting" value="correct">
  <label for="prompting-correct" class="quiz-option-prompting" data-correct="true">🎭 System prompts define the assistant's role, user prompts are the actual questions or inputs</label>
  
  <input type="radio" name="quiz-prompting" id="prompting-wrong2" class="quiz-radio-prompting" value="wrong">
  <label for="prompting-wrong2" class="quiz-option-prompting">📊 Both system and user prompts serve the same purpose and can be used interchangeably</label>

  <div class="feedback-prompting success">✅ <strong>Perfect!</strong> System prompts establish the AI's behavior and role, while user prompts provide the specific task or question.</div>
  <div class="feedback-prompting error">❌ <strong>Try again!</strong> Think about how system prompts set the context and user prompts provide the specific input.</div>
</div>

</div>

[⬅️ Previous: Prompting](#-prompting-how-to-guide-the-model) | [➡️ Next: Hallucinations](#-hallucinations-when-models-make-stuff-up)

---

## 🚨 Hallucinations: When Models Make Stuff Up

[🔝 Back to Contents](#contents)

LLMs sometimes **hallucinate** — confidently generate text that’s incorrect or fictional.

Why it happens:
- They optimize for coherence, not factual accuracy
- They don't “know” facts—they predict likely token sequences

Mitigation tips:
- Include accurate facts in the prompt
- Use guardrails (see below)
- Add retrieval or validation layers

So let's do a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">🚨 Quiz: Why do LLMs hallucinate?</h3>

<style>
.quiz-container-hallucination { position: relative; }
.quiz-option-hallucination {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-hallucination:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-hallucination { display: none; }
.quiz-radio-hallucination:checked + .quiz-option-hallucination { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-hallucination[value="wrong"]:checked + .quiz-option-hallucination { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-hallucination {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#hallucination-correct:checked ~ .feedback-hallucination.success { opacity: 1; }
#hallucination-wrong1:checked ~ .feedback-hallucination.error, #hallucination-wrong2:checked ~ .feedback-hallucination.error { opacity: 1; }
.feedback-hallucination.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-hallucination.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-hallucination">

  <input type="radio" name="quiz-hallucination" id="hallucination-wrong1" class="quiz-radio-hallucination" value="wrong">
  <label for="hallucination-wrong1" class="quiz-option-hallucination">🐛 It's a bug that will be fixed in future models</label>

  <input type="radio" name="quiz-hallucination" id="hallucination-wrong2" class="quiz-radio-hallucination" value="wrong">
  <label for="hallucination-wrong2" class="quiz-option-hallucination">💾 They have corrupted training data</label>
  
  <input type="radio" name="quiz-hallucination" id="hallucination-correct" class="quiz-radio-hallucination" value="correct">
  <label for="hallucination-correct" class="quiz-option-hallucination" data-correct="true">🎯 They optimize for coherence and predict likely token sequences, not factual accuracy</label>

  <div class="feedback-hallucination success">✅ <strong>Excellent!</strong> LLMs generate plausible-sounding text based on patterns, not facts they "know" to be true.</div>
  <div class="feedback-hallucination error">❌ <strong>Try again!</strong> Think about how LLMs fundamentally work - they predict tokens, not retrieve facts.</div>
</div>

</div>

[⬅️ Previous: Hallucinations](#-hallucinations-when-models-make-stuff-up) | [➡️ Next: Guardrails](#️-guardrails-controlling-what-models-say)

---

## 🛡️ Guardrails: Controlling What Models Say

[🔝 Back to Contents](#contents)

To keep your assistant safe and on-task, you can apply **guardrails**, such as:
- Prompt templates with strict instructions
- Output filters (block offensive or harmful content)
- External validation (e.g., fact-checking or classifiers)

For Canopy AI, these guardrails are essential to ensure alignment with educational standards.

So let's do a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">🛡️ Quiz: What are guardrails in AI systems?</h3>

<style>
.quiz-container-guardrails { position: relative; }
.quiz-option-guardrails {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-guardrails:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-guardrails { display: none; }
.quiz-radio-guardrails:checked + .quiz-option-guardrails { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-guardrails[value="wrong"]:checked + .quiz-option-guardrails { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-guardrails {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#guardrails-correct:checked ~ .feedback-guardrails.success { opacity: 1; }
#guardrails-wrong1:checked ~ .feedback-guardrails.error, #guardrails-wrong2:checked ~ .feedback-guardrails.error { opacity: 1; }
.feedback-guardrails.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-guardrails.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-guardrails">

  <input type="radio" name="quiz-guardrails" id="guardrails-wrong1" class="quiz-radio-guardrails" value="wrong">
  <label for="guardrails-wrong1" class="quiz-option-guardrails">🚀 Performance optimizations that make models run faster</label>

  <input type="radio" name="quiz-guardrails" id="guardrails-correct" class="quiz-radio-guardrails" value="correct">
  <label for="guardrails-correct" class="quiz-option-guardrails" data-correct="true">🛡️ Safety mechanisms that control and constrain model behavior and outputs</label>
  
  <input type="radio" name="quiz-guardrails" id="guardrails-wrong2" class="quiz-radio-guardrails" value="wrong">
  <label for="guardrails-wrong2" class="quiz-option-guardrails">📊 Data preprocessing techniques used during model training</label>

  <div class="feedback-guardrails success">✅ <strong>Excellent!</strong> Guardrails are essential safety measures that help ensure AI systems behave appropriately and safely in production environments.</div>
  <div class="feedback-guardrails error">❌ <strong>Try again!</strong> Think about how guardrails help control what an AI system can and cannot do or say.</div>
</div>

</div>

[⬅️ Previous: Guardrails](#️-guardrails-controlling-what-models-say) | [➡️ Next: Key Inference Metrics](#-key-inference-metrics)

---

## 📊 Key Inference Metrics

[🔝 Back to Contents](#contents)

[🔝 Back to Contents](#contents)

Understanding how your model performs helps you scale and troubleshoot.

| Metric                  | Meaning                                                      |
|-------------------------|--------------------------------------------------------------|
| **TTFT**                | Time to First Token – how fast the model starts responding   |
| **TPOT**                | Time Per Output Token – how fast each new token is generated |
| **Throughput**          | Number of parallel requests handled                          |
| **VRAM Usage**          | GPU memory required (↑ model size or context = ↑ memory)     |

These metrics help you balance latency vs. cost in OpenShift AI deployments.

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">📏 Quiz: What does TTFT measure in LLM inference?</h3>

<style>
.quiz-container-metrics { position: relative; }
.quiz-option-metrics {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-metrics:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-metrics { display: none; }
.quiz-radio-metrics:checked + .quiz-option-metrics { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-metrics[value="wrong"]:checked + .quiz-option-metrics { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-metrics {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#metrics-correct:checked ~ .feedback-metrics.success { opacity: 1; }
#metrics-wrong1:checked ~ .feedback-metrics.error, #metrics-wrong2:checked ~ .feedback-metrics.error { opacity: 1; }
.feedback-metrics.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-metrics.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-metrics">

  <input type="radio" name="quiz-metrics" id="metrics-wrong1" class="quiz-radio-metrics" value="wrong">
  <label for="metrics-wrong1" class="quiz-option-metrics">💾 The total amount of VRAM memory consumed during inference</label>

  <input type="radio" name="quiz-metrics" id="metrics-wrong2" class="quiz-radio-metrics" value="wrong">
  <label for="metrics-wrong2" class="quiz-option-metrics">📊 The number of tokens processed per second during generation</label>
  
  <input type="radio" name="quiz-metrics" id="metrics-correct" class="quiz-radio-metrics" value="correct">
  <label for="metrics-correct" class="quiz-option-metrics" data-correct="true">⚡ How long it takes for the model to generate its first response token</label>

  <div class="feedback-metrics success">✅ <strong>Perfect!</strong> TTFT (Time to First Token) measures the initial latency before the model starts responding, which is crucial for user experience.</div>
  <div class="feedback-metrics error">❌ <strong>Not quite!</strong> TTFT specifically measures the time delay before the model produces its first output token.</div>
</div>

</div>

[⬅️ Previous: Key Inference Metrics](#-key-inference-metrics) | [➡️ Next: Model Sizes and GPU Needs](#-model-sizes-and-gpu-needs)

---

## 📦 Model Sizes and GPU Needs

[🔝 Back to Contents](#contents)

Model size matters—for performance *and* capability.

| Model Size     | Parameters | GPU Requirement            | Notes                            |
|----------------|------------|-----------------------------|----------------------------------|
| **<3B**         | Small      | 8–12GB VRAM (1 GPU)         | Lightweight and fast             |
| **7B–13B**      | Medium     | ≥24GB VRAM or quantization  | Balanced power vs. cost          |
| **>30B**        | Large      | Multi-GPU or high-end cards | Slower, but more context-aware   |

🧠 Larger models may be smarter, but smaller ones are often faster and easier to deploy.

Let's test your understanding with a quiz!

<div style="background: linear-gradient(135deg, #e8f2ff 0%, #f5e6ff 100%); padding: 20px; border-radius: 10px; margin: 20px 0; border: 1px solid #d1e7dd;">

<h3 style="color: #5a5a5a; margin-top: 0;">📦 Quiz: What's the trade-off with model sizes?</h3>

<style>
.quiz-container-models { position: relative; }
.quiz-option-models {
  display: block;
  margin: 8px 0;
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
  border: 2px solid #e9ecef;
  color: #495057;
}
.quiz-option-models:hover { background: #fff; transform: translateY(-1px); border-color: #dee2e6; }
.quiz-radio-models { display: none; }
.quiz-radio-models:checked + .quiz-option-models { background: #d4edda; color: #155724; border-color: #c3e6cb; }
.quiz-radio-models[value="wrong"]:checked + .quiz-option-models { background: #f8d7da; color: #721c24; border-color: #f5c6cb; }
.feedback-models {
  margin-top: 15px;
  padding: 12px;
  border-radius: 6px;
  opacity: 0;
  transition: opacity 0.3s ease;
}
#models-correct:checked ~ .feedback-models.success { opacity: 1; }
#models-wrong1:checked ~ .feedback-models.error, #models-wrong2:checked ~ .feedback-models.error { opacity: 1; }
.feedback-models.success { background: #d1f2eb; color: #0c5d56; border: 1px solid #a3d9cc; }
.feedback-models.error { background: #fce8e6; color: #58151c; border: 1px solid #f5b7b1; }
</style>

<div class="quiz-container-models">

  <input type="radio" name="quiz-models" id="models-wrong1" class="quiz-radio-models" value="wrong">
  <label for="models-wrong1" class="quiz-option-models">📈 Larger models are always better and should be chosen when possible</label>

  <input type="radio" name="quiz-models" id="models-wrong2" class="quiz-radio-models" value="wrong">
  <label for="models-wrong2" class="quiz-option-models">⚡ Model size doesn't affect hardware requirements</label>
  
  <input type="radio" name="quiz-models" id="models-correct" class="quiz-radio-models" value="correct">
  <label for="models-correct" class="quiz-option-models" data-correct="true">⚖️ Larger models may be more capable but require more GPU resources and are slower</label>

  <div class="feedback-models success">✅ <strong>Great understanding!</strong> Model selection involves balancing capability with resource constraints and performance needs.</div>
  <div class="feedback-models error">❌ <strong>Not quite!</strong> Consider the trade-offs between model capability and deployment requirements.</div>
</div>

</div>

[⬅️ Previous: Model Sizes and GPU Needs](#-model-sizes-and-gpu-needs) | [➡️ Next: Summary](#-summary)

---

## ✅ Summary

[🔝 Back to Contents](#contents)

| Concept                | Key Idea                                                            |
|------------------------|---------------------------------------------------------------------|
| **Token**              | The basic unit of LLM input/output                                  |
| **Next-token machine** | LLMs predict one token at a time                                    |
| **Attention**          | Helps models focus on relevant words                                |
| **Context length**     | How much the model can "remember"                                   |
| **KV Cache**           | Speeds up generation by caching internal state                      |
| **Prompting**          | Guides model behavior through smart input design                    |
| **Hallucination**      | LLMs can generate plausible but wrong info                          |
| **Guardrails**         | Techniques to constrain model behavior and output                   |
| **TTFT & TPOT**        | Speed metrics for user experience                                   |
| **VRAM & Throughput**  | Resource and scalability metrics                                    |
| **Model size & GPU**   | Match model size to hardware capability and use case                |

[⬅️ Previous: Key Inference Metrics](#-key-inference-metrics) | [🔝 Back to Contents](#contents)

---

[🔝 Back to Contents](#contents)

## 📚 References

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information processing systems*, 30. This seminal paper introduced the Transformer architecture and self-attention mechanism that forms the foundation of modern large language models like GPT, BERT, and others used in AI applications today.

[🔝 Return to Top](#-deep-dive-how-llms-generate-text)

