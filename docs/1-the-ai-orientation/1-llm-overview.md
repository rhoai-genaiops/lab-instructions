# Understanding LLMs: A Deep Dive

In Canopy, your assistant feels fast and natural but whatâ€™s really happening behind the scenes? This guide breaks down the key mechanics that make large language models (LLMs) work while you're chatting with them.

We'll focus on the concepts that matter most for real-world deployments. 

Use the index below to explore specific topics or revisit areas you'd like to review.

## Guide Structure

This guide is divided into four main sections:

1. [ðŸ§± LLM Fundamentals](1-llm-fundamentals.md)
   - Understanding tokens
   - How LLMs maintain state
   - Next-token prediction process
  
2. [ðŸ’­ Using and Controlling LLMs](3-llm-usage-control.md)
   - Prompting techniques
   - Handling hallucinations
   - Implementing guardrails
  
3. [ðŸ§  Memory and Processing](2-llm-memory-processing.md)
   - Attention mechanism
   - Context windows
   - KV Cache optimization

4. [ðŸ“Š Performance and Hardware](4-llm-performance.md)
   - Key performance metrics
   - Model sizes and requirements
   - Hardware considerations


## ðŸŽ¯ Learning Objectives

After completing this guide, you will understand:

- How LLMs process and generate text
- Key factors affecting LLM performance
- Hardware considerations for deployment
