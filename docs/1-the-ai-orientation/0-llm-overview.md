# Understanding LLMs: A Deep Dive

In Canopy AI, your assistant appears to think quickly and respond naturally. But what's actually happening behind the scenes? This guide explores the core mechanics that power LLMs during **inference** â€” the process of turning prompts into meaningful, human-like output.

We'll focus on the concepts that matter most for real-world deployment in educational settings.

## Guide Structure

This guide is divided into four main sections:

1. [ðŸ§± LLM Fundamentals](1-llm-fundamentals.md)
   - Understanding tokens
   - How LLMs maintain state
   - Next-token prediction process

2. [ðŸ§  Memory and Processing](2-llm-memory-processing.md)
   - Attention mechanism
   - Context windows
   - KV Cache optimization

3. [ðŸ’­ Using and Controlling LLMs](3-llm-usage-control.md)
   - Prompting techniques
   - Handling hallucinations
   - Implementing guardrails

4. [ðŸ“Š Performance and Hardware](4-llm-performance.md)
   - Key performance metrics
   - Model sizes and requirements
   - Hardware considerations


## ðŸŽ¯ Learning Objectives

After completing this guide, you will understand:
- How LLMs process and generate text
- Key factors affecting LLM performance
- Best practices for prompting and control
- Hardware requirements for deployment
